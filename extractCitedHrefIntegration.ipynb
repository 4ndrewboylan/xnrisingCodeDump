{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from xml.etree import ElementTree\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in the perfect world part\n",
    "# dom = ElementTree.parse(\"pubmed22n0001.xml\")\n",
    "# Articles = dom.findall('PubmedArticle/MedlineCitation/Article')\n",
    "# pubmedID = dom.findall('PubmedArticle/PubmedData/ArticleIdList/')\n",
    "# allTitles = []\n",
    "# allAuthors = []\n",
    "# # indexes = []\n",
    "# IDs = []\n",
    "\n",
    "# for i, eachArticles in enumerate(Articles):\n",
    "#     # index.append(i+1)\n",
    "#     title = eachArticles.find('ArticleTitle').text\n",
    "#     allTitles.append(title)\n",
    "#     allArticleAuthors = []\n",
    "#     # for eachId in eachArticles.findall()\n",
    "#     for authors in eachArticles.findall('AuthorList/Author'):\n",
    "#         if authors.find('LastName') is not None:\n",
    "#             LastName = authors.find('LastName').text\n",
    "#         else:\n",
    "#             LastName = \"NULL\"\n",
    "#         if authors.find('ForeName') is not None:\n",
    "#             ForeName = authors.find('ForeName').text\n",
    "#         else:\n",
    "#             ForeName = \"NULL\"\n",
    "#         allArticleAuthors.append(ForeName + \" \" + LastName)\n",
    "#     allAuthors.append(allArticleAuthors)\n",
    "# for i, each in enumerate(pubmedID):\n",
    "#     # indexes.append(i+1)\n",
    "#     if (each.attrib == {'IdType': 'pubmed'}) is not None:\n",
    "#         if each.attrib == {'IdType': 'pubmed'}:\n",
    "#             IDs.append(each.text)\n",
    "#             # indexes.append(i+1)\n",
    "#     else:\n",
    "#         IDs.append(\"NULLID\")\n",
    "#         # indexes.append(i+1)\n",
    "\n",
    "# components = {\n",
    "#     'Title' : allTitles,\n",
    "#     'Author': allAuthors,\n",
    "#     # 'Indexes' : indexes,\n",
    "#     'PubMedIDs' : IDs\n",
    "# }\n",
    "\n",
    "# OriDf = pd.DataFrame(components)\n",
    "# customString = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "# endCString = \"/citations/\"\n",
    "# styles = [\"ama\", \"apa\", \"mla\", \"nlm\"]\n",
    "\n",
    "# ama, apa, mla, nlm = [], [], [], []\n",
    "# # for i, eachId in enumerate(IDs):\n",
    "# errorList = []\n",
    "# for eachId in IDs:\n",
    "#     try:\n",
    "#         tempMatch = requests.get(customString + eachId + endCString).text\n",
    "#         matchDict = eval(tempMatch)\n",
    "#         try:\n",
    "#             ama.append(matchDict[\"ama\"]['orig'])\n",
    "#         except:\n",
    "#             ama.append(\"NULL\")\n",
    "#         try:\n",
    "#             apa.append(matchDict[\"apa\"]['orig'])\n",
    "#         except:\n",
    "#             apa.append(\"NULL\")\n",
    "#         try:\n",
    "#             mla.append(matchDict[\"mla\"]['orig'])\n",
    "#         except:\n",
    "#             mla.append(\"NULL\")\n",
    "#         try:\n",
    "#             nlm.append(matchDict[\"nlm\"]['orig'])\n",
    "#         except:\n",
    "#             nlm.append(\"NULL\")\n",
    "#     except:\n",
    "#         errorList.append(eachId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life part\n",
    "customString = \"https://pubmed.ncbi.nlm.nih.gov\"\n",
    "refString = \"references/\"\n",
    "holderIDs = [x for x in range(30000)]\n",
    "citedByHrefDict = defaultdict(list)\n",
    "citedString = defaultdict(list)\n",
    "realisticCitation = defaultdict(list)\n",
    "\n",
    "for each in holderIDs:\n",
    "    try:\n",
    "        tempRawPage = requests.get(customString + \"/\" + str(each) + \"/\").text\n",
    "        soup = bs(tempRawPage, \"html.parser\")\n",
    "        tempCitedByList = soup.find('div', class_= 'citedby-articles').find_all('a', class_= 'docsum-title')\n",
    "        for i, eachCitation in enumerate(tempCitedByList):\n",
    "            citedByHrefDict[each].append(eachCitation['href'])\n",
    "            # take max 3 references per article or adjust to your desired amount\n",
    "            # the more x references, the more columns to be added to the df,\n",
    "            # the better we can access our performance, \n",
    "            # this is especially important since many has little to none reference <- good compensation\n",
    "            if i == 2:\n",
    "                break\n",
    "    except:\n",
    "        citedByHrefDict[each] = \"Nan\"\n",
    "\n",
    "print(\"Done with citedByHrefDict population\")\n",
    "timerPercentage = 1\n",
    "for eachDictEntryKey in citedByHrefDict.keys():\n",
    "    if eachDictEntryKey // 300 == 0:\n",
    "        print(timerPercentage)\n",
    "        timerPercentage+=1\n",
    "    if citedByHrefDict[eachDictEntryKey] == 'Nan':\n",
    "        for i in range(3):\n",
    "            realisticCitation[eachDictEntryKey].append('Nan')\n",
    "        continue\n",
    "    for eachHref in citedByHrefDict[eachDictEntryKey]:\n",
    "        tempRawPage = requests.get(customString + eachHref + refString).text\n",
    "        soup = bs(tempRawPage, \"html.parser\")\n",
    "        tempCitedByList = soup.find_all('li', class_= 'skip-numbering')\n",
    "        for each in tempCitedByList: # tempCitedByList has all references on that article\n",
    "            # there are cases where there are more than 1 anchor tag per entry\n",
    "            tempHrefCheck = each.find_all('a', href = True)\n",
    "            if(tempHrefCheck is not None):\n",
    "                # this loop is here because the actual link might not be the first anchor tag\n",
    "                if len(realisticCitation[eachDictEntryKey]) == 3:\n",
    "                    break\n",
    "                for eachTempHrefCheck in tempHrefCheck:\n",
    "                    if eachTempHrefCheck[\"href\"] == \"/\" + str(eachDictEntryKey) + \"/\":\n",
    "                        realisticCitation[eachDictEntryKey].append(each.contents[0].strip().split(\"\\n\")[0])\n",
    "            else:\n",
    "                print(each, tempCitedByList)\n",
    "\n",
    "print(\"Done with realisticCitation population\")\n",
    "for each in realisticCitation:\n",
    "    if len(realisticCitation[each]) < 3:\n",
    "        for i in range(3 - len(realisticCitation[each])):\n",
    "            realisticCitation[each].append(\"Nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDf = pd.DataFrame(realisticCitation).T\n",
    "# bigDf = pd.concat([OriDf, realDf],axis=1)\n",
    "# bigDf = bigDf[~bigDf['PubMedIDs'].isin(errorList)]\n",
    "# bigDf.reset_index(inplace=True)\n",
    "# rawStringDf = pd.DataFrame(list(zip(ama,apa,mla,nlm)), columns = [\"ama\",\"apa\",\"mla\",\"nlm\"])\n",
    "# df = pd.concat([bigDf, rawStringDf],axis=1)\n",
    "# df.drop([\"index\"], axis = 1, inplace=True)\n",
    "# df.to_pickle(\"first30k.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
